{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4aa2a8",
   "metadata": {},
   "source": [
    "# Machine Learning Evaluation & Tuning Concepts\n",
    "\n",
    "## 1. Cross-Validation (CV)\n",
    "\n",
    "**What it is:**\n",
    "Cross-validation is a statistical method used to estimate the performance of a machine learning model on unseen data. It partitions the dataset into multiple subsets (\"folds\") and rotates through them for training and validation.\n",
    "\n",
    "**Why it matters:**\n",
    "Using a single train-test split can give unreliable performance estimates due to random variation. Cross-validation mitigates this by averaging performance across multiple splits, reducing variance.\n",
    "\n",
    "**Common Types:**\n",
    "\n",
    "- **K-Fold CV:** Splits the data into *k* parts. Trains on *k-1* and tests on the 1 remaining. Repeats *k* times.\n",
    "- **Stratified K-Fold CV:** Like K-Fold, but maintains class distribution across folds. Crucial for classification with imbalanced data.\n",
    "- **Leave-One-Out CV (LOOCV):** Uses one observation as test data and the rest as training. High variance, expensive.\n",
    "- **Repeated K-Fold CV:** Repeats K-Fold multiple times with different splits for a more robust estimate.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ROC-AUC (Receiver Operating Characteristic – Area Under Curve)\n",
    "\n",
    "**What it is:**\n",
    "ROC-AUC is a metric used to evaluate classification models by measuring how well a model ranks positive vs negative instances.\n",
    "\n",
    "- **ROC Curve:** Graph plotting True Positive Rate (TPR) vs False Positive Rate (FPR) at various thresholds.\n",
    "- **AUC:** Scalar value measuring the area under the ROC curve. Represents probability that a randomly chosen positive instance is ranked higher than a negative one.\n",
    "\n",
    "**Why it matters:**\n",
    "ROC-AUC is threshold-independent and is a better indicator than accuracy, especially with imbalanced datasets.\n",
    "\n",
    "**Interpretation:**\n",
    "- AUC = 1.0 → Perfect model\n",
    "- AUC = 0.5 → No better than random\n",
    "- AUC < 0.5 → Worse than random\n",
    "\n",
    "---\n",
    "\n",
    "## 3. GridSearchCV\n",
    "\n",
    "**What it is:**\n",
    "GridSearchCV is a hyperparameter tuning method that exhaustively tries all possible combinations from a predefined grid of parameter values.\n",
    "\n",
    "**How it works:**\n",
    "Each combination is evaluated using cross-validation. The model with the best average performance across folds is selected.\n",
    "\n",
    "**Why it matters:**\n",
    "Tuning hyperparameters can drastically improve model performance. GridSearch automates this in a systematic way.\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive\n",
    "- Evaluates all combinations, which may be inefficient for large spaces\n",
    "\n",
    "---\n",
    "\n",
    "## 4. RandomizedSearchCV\n",
    "\n",
    "**What it is:**\n",
    "RandomizedSearchCV is a hyperparameter tuning method that randomly samples a fixed number of parameter combinations from given distributions.\n",
    "\n",
    "**How it works:**\n",
    "Instead of exhaustively searching, it selects random combinations and evaluates them with cross-validation.\n",
    "\n",
    "**Why it matters:**\n",
    "Efficient when the search space is large or when computational resources are limited. Useful for quick and broad exploration.\n",
    "\n",
    "**Advantages:**\n",
    "- Faster than GridSearchCV\n",
    "- Can explore a larger space\n",
    "- Supports continuous distributions\n",
    "\n",
    "**Limitations:**\n",
    "- Results depend on randomness\n",
    "- Might miss the optimal combination\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
