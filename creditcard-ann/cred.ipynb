{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120b2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21c4768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('creditcard.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f6db80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data.drop(columns=['Class'])\n",
    "y=data['Class']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3238480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126cfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26d7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Input(shape=(30,)))\n",
    "model.add(Dense(50,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(120,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719a09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee507de",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=12,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545ad2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "check=ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa8dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60e7c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3308/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 42.0865\n",
      "Epoch 1: val_loss did not improve from 6.26806\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 28.5433 - val_accuracy: 0.9982 - val_loss: 10.3361 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m3326/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 22.5436\n",
      "Epoch 2: val_loss did not improve from 6.26806\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 17.0715 - val_accuracy: 0.9982 - val_loss: 19.4678 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m3331/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 7.8697\n",
      "Epoch 3: val_loss improved from 6.26806 to 5.94462, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 5.2118 - val_accuracy: 0.9982 - val_loss: 5.9446 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m3336/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 2.4128\n",
      "Epoch 4: val_loss improved from 5.94462 to 0.50760, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 1.7051 - val_accuracy: 0.9982 - val_loss: 0.5076 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m3314/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 3.2275\n",
      "Epoch 5: val_loss did not improve from 0.50760\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 3.4380 - val_accuracy: 0.9983 - val_loss: 0.5170 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.9925\n",
      "Epoch 6: val_loss did not improve from 0.50760\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 1.4346 - val_accuracy: 0.9982 - val_loss: 5.1459 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m3332/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 1.9899\n",
      "Epoch 7: val_loss improved from 0.50760 to 0.05900, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 1.4065 - val_accuracy: 0.9985 - val_loss: 0.0590 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m3333/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0966\n",
      "Epoch 8: val_loss improved from 0.05900 to 0.01606, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0898 - val_accuracy: 0.9986 - val_loss: 0.0161 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m3324/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0631\n",
      "Epoch 9: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0572 - val_accuracy: 0.9983 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m3305/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0156\n",
      "Epoch 10: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0170 - val_accuracy: 0.9982 - val_loss: 0.0986 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m3310/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0256\n",
      "Epoch 11: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0186 - val_accuracy: 0.9982 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m3333/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0226\n",
      "Epoch 12: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0210 - val_accuracy: 0.9982 - val_loss: 0.0201 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m3326/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0143\n",
      "Epoch 13: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0351 - val_accuracy: 0.9982 - val_loss: 0.0207 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m3334/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0174\n",
      "Epoch 14: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0186 - val_accuracy: 0.9982 - val_loss: 0.0191 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m3318/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0151\n",
      "Epoch 15: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0194 - val_accuracy: 0.9982 - val_loss: 0.0199 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m3323/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0191\n",
      "Epoch 16: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0192 - val_accuracy: 0.9982 - val_loss: 0.0221 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m3330/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0164\n",
      "Epoch 17: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0253 - val_accuracy: 0.9982 - val_loss: 0.0234 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m3322/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0144\n",
      "Epoch 18: val_loss did not improve from 0.01606\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0176 - val_accuracy: 0.9982 - val_loss: 0.0206 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m3325/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0160\n",
      "Epoch 19: val_loss did not improve from 0.01606\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0167 - val_accuracy: 0.9982 - val_loss: 0.0170 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m3306/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0184\n",
      "Epoch 20: val_loss improved from 0.01606 to 0.01384, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0164 - val_accuracy: 0.9982 - val_loss: 0.0138 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m3327/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0125\n",
      "Epoch 21: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0137 - val_accuracy: 0.9982 - val_loss: 0.0169 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m3337/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0153\n",
      "Epoch 22: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0159 - val_accuracy: 0.9982 - val_loss: 0.0180 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m3319/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0172\n",
      "Epoch 23: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0163 - val_accuracy: 0.9982 - val_loss: 0.0177 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m3305/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0155\n",
      "Epoch 24: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0131 - val_accuracy: 0.9983 - val_loss: 0.0172 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m3318/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0155\n",
      "Epoch 25: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0157 - val_accuracy: 0.9983 - val_loss: 0.0177 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m3307/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9983 - loss: 0.0132\n",
      "Epoch 26: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0146 - val_accuracy: 0.9983 - val_loss: 0.0147 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0135\n",
      "Epoch 27: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0130 - val_accuracy: 0.9984 - val_loss: 0.0150 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m3314/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0135\n",
      "Epoch 28: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0129 - val_accuracy: 0.9983 - val_loss: 0.0162 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m3329/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0120\n",
      "Epoch 29: val_loss did not improve from 0.01384\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0145 - val_accuracy: 0.9985 - val_loss: 0.0146 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m3317/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0127\n",
      "Epoch 30: val_loss improved from 0.01384 to 0.01298, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0130 - val_accuracy: 0.9984 - val_loss: 0.0130 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m3336/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0425\n",
      "Epoch 31: val_loss did not improve from 0.01298\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0635 - val_accuracy: 0.9984 - val_loss: 0.0139 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m3335/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0116\n",
      "Epoch 32: val_loss improved from 0.01298 to 0.01292, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0138 - val_accuracy: 0.9984 - val_loss: 0.0129 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m3334/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0171\n",
      "Epoch 33: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9984 - loss: 0.0208 - val_accuracy: 0.9983 - val_loss: 0.0158 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m3323/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0125\n",
      "Epoch 34: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0136 - val_accuracy: 0.9982 - val_loss: 0.0141 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m3328/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0123\n",
      "Epoch 35: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0119 - val_accuracy: 0.9984 - val_loss: 0.0142 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m3314/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0133\n",
      "Epoch 36: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0133 - val_accuracy: 0.9983 - val_loss: 0.0134 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m3326/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0111\n",
      "Epoch 37: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0117 - val_accuracy: 0.9985 - val_loss: 0.0143 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m3329/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0120\n",
      "Epoch 38: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0134 - val_accuracy: 0.9984 - val_loss: 0.0139 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m3316/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0132\n",
      "Epoch 39: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0222 - val_accuracy: 0.9982 - val_loss: 0.0152 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m3318/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0111\n",
      "Epoch 40: val_loss did not improve from 0.01292\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0119 - val_accuracy: 0.9983 - val_loss: 0.0157 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m3310/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0223\n",
      "Epoch 41: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0170 - val_accuracy: 0.9984 - val_loss: 0.0146 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m3329/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0096\n",
      "Epoch 42: val_loss did not improve from 0.01292\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0090 - val_accuracy: 0.9987 - val_loss: 0.0146 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m3325/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0073\n",
      "Epoch 43: val_loss improved from 0.01292 to 0.00956, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0085 - val_accuracy: 0.9987 - val_loss: 0.0096 - learning_rate: 2.5000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m3325/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0066\n",
      "Epoch 44: val_loss did not improve from 0.00956\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0060 - val_accuracy: 0.9984 - val_loss: 0.0167 - learning_rate: 2.5000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m3306/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0078\n",
      "Epoch 45: val_loss did not improve from 0.00956\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0078 - val_accuracy: 0.9984 - val_loss: 0.0124 - learning_rate: 2.5000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m3328/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0084\n",
      "Epoch 46: val_loss improved from 0.00956 to 0.00784, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0077 - val_accuracy: 0.9987 - val_loss: 0.0078 - learning_rate: 2.5000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m3335/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0083\n",
      "Epoch 47: val_loss improved from 0.00784 to 0.00652, saving model to best_model.keras\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0096 - val_accuracy: 0.9989 - val_loss: 0.0065 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m3311/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0072\n",
      "Epoch 48: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0076 - val_accuracy: 0.9987 - val_loss: 0.0086 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m3316/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0056\n",
      "Epoch 49: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0076 - val_accuracy: 0.9987 - val_loss: 0.0071 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m3333/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0054\n",
      "Epoch 50: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0055 - val_accuracy: 0.9988 - val_loss: 0.0209 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0246\n",
      "Epoch 51: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0151 - val_accuracy: 0.9992 - val_loss: 0.0078 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m3314/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0100\n",
      "Epoch 52: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0096 - val_accuracy: 0.9953 - val_loss: 0.0211 - learning_rate: 2.5000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m3316/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0056\n",
      "Epoch 53: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0060 - val_accuracy: 0.9986 - val_loss: 0.0087 - learning_rate: 2.5000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m3331/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0065\n",
      "Epoch 54: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0061 - val_accuracy: 0.9985 - val_loss: 0.0118 - learning_rate: 2.5000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m3303/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0078\n",
      "Epoch 55: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0077 - val_accuracy: 0.9987 - val_loss: 0.0104 - learning_rate: 2.5000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m3324/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0080\n",
      "Epoch 56: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0088 - val_accuracy: 0.9988 - val_loss: 0.0079 - learning_rate: 2.5000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m3311/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0053\n",
      "Epoch 57: val_loss did not improve from 0.00652\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0052 - val_accuracy: 0.9990 - val_loss: 0.0096 - learning_rate: 2.5000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m3310/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0053\n",
      "Epoch 58: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0050 - val_accuracy: 0.9988 - val_loss: 0.0078 - learning_rate: 1.2500e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0047\n",
      "Epoch 59: val_loss did not improve from 0.00652\n",
      "\u001b[1m3338/3338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0044 - val_accuracy: 0.9988 - val_loss: 0.0087 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "model_hist=model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=64,callbacks=[es,check,lr],epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c89ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model_hist.history['loss'],label='training')\n",
    "plt.plot(model_hist.history['val_loss'],label='testing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
